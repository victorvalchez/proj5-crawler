#!/usr/bin/env python3

import argparse
import socket
from urllib.parse import urlparse
from html.parser import HTMLParser
import ssl

from collections import deque

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443
CRLF = "\r\n"

BUFFER = 10000

# HTTP Status Codes
SUCCESS = 200
FOUND = 302
FORBIDDEN = 403
NOT_FOUND = 404
ERROR = 503

class CustomHTMLParser(HTMLParser):
    def __init__(self, crawler):
        HTMLParser.__init__(self)
        self.crawler = crawler

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for key, value in attrs:
                if key == 'href':
                    self.crawler.feed_queue(value)
        
    def handle_data(self, data):
        if "FLAG: " in data:
            self.crawler.add_flag(data.split(": ")[1])
	


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
      
        self.csrftoken = None
        self.sessionid = None

        self.visited = set() #set of pages already crawled or pending to crawl
        self.queue = deque() #queue of pages pending to crawl
		
        self.flags = []

    def feed_queue(self, link):
		# we only crawl URLs on the specified server
        if "/fakebook/" in link:
            full_link = f"http://{self.server}{link}"
			
            if full_link not in self.visited:
                self.queue.append(full_link)
                #value is in the queue so there's no need to add it again in the future
                self.visited.add(full_link) 
				

    def add_flag(self, flag):
        self.flags.add(flag)
		

    def parse_response(self, res):
        """ Parse raw HTTP response. """
        res = res.strip().split(2 * CRLF)
        response = dict()
        response["body"] = res[-1] if len(res) > 1 else None
        status_and_headers = res[0].split(CRLF)

        initial_response_line = status_and_headers[0]
        status = int(initial_response_line.split(" ")[1])
        response["status"] = status

        response["headers"] = dict()
        response["cookies"] = list()
        headers = status_and_headers[1:]
        for header in headers:
            (key, value) = header.split(": ")
            if key == "Set-Cookie":
                response["cookies"].append(value)
            else:
                response["headers"][key] = value

        return response

    def login(self):
        login_form = f"http://{self.server}:{self.port}/accounts/login/?next=/fakebook/"
        get_response_str = self.send_get_and_receive(login_form)
        get_respond = self.parse_response(get_response_str)

		# Get CSRF token cookie.
        for cookie in get_respond["cookies"]:
            if "csrftoken" in cookie:
                self.csrftoken = cookie.split("; ")[0].split("=")[1]
        assert self.csrftoken is not None, "ERROR: Failed to get CSRF token from login page."

        # Log in and get session cookie.
        login_content = f"username={self.username}&password={self.password}&csrfmiddlewaretoken={self.csrftoken}&next="
        send_login = self.send_login(login_form, login_content)

        print(send_login)
        send_login_parsed = self.parse_response(send_login)
        

        for cookie in send_login_parsed["cookies"]:
            if "sessionid" in cookie:
                self.sessionid = cookie.split("; ")[0].split("=")[1]
        assert self.sessionid is not None, "ERROR: Failed to get Session ID after login."
		

    def send_get_and_receive(self, url):
        # Check if we can access that domain
        if self.server not in url:
          return -1  # '[ERROR]: Cannot Crawl Outside The Target Domain'
			
        url_parsed = urlparse(url)

        get_request = f"GET {url_parsed.path} HTTP/1.1{CRLF}"
        host = f"Host: {url_parsed.netloc}{CRLF}"

        # Add cookies.
        request = f"{get_request}{host}{CRLF}"
        if self.csrftoken is not None and self.sessionid is not None:
            cookies = f"csrftoken={self.csrftoken}; sessionid={self.sessionid}"
            cookie_header = f"Cookie: {cookies}{CRLF}"
            request = f"{get_request}{host}{cookie_header}{CRLF}"

        so = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock = ssl.wrap_socket(so)
        sock.connect((self.server, self.port))
        sock.send(request.encode())
        response_data = sock.recv(BUFFER).decode()
        sock.close()

        return response_data

    def send_login(self, url, content):
        """ Send a HTTP/1.1 POST request. """
        if self.server not in url:
            return -1 # '[ERROR]: Cannot Crawl Outside The Target Domain'
        url = urlparse(url)

        # Create HTTP initial request line and headers.
        post_request = f"POST {url.path} HTTP/1.1{CRLF}"
        host = f"Host: {url.netloc}{CRLF}"
        from_line = f"From: valenciasanchez.v@northeastern.edu{CRLF}"
        user_agent = f"User-Agent: cs3700-webcrawler/1.0{CRLF}"
        content_type = f"Content-Type: application/x-www-form-urlencoded{CRLF}"
        content_length = f"Content-Length: {len(content)}{CRLF}"

        # Add cookies.
        request = f"{post_request}{host}{from_line}{user_agent}{content_type}{content_length}{CRLF}{content}"
        if self.csrftoken is not None and self.sessionid is not None:
            cookies = f"csrftoken={self.csrftoken}; sessionid={self.sessionid}"
            cookie_header = f"Cookie: {cookies}{CRLF}"
            request = f"{post_request}{host}{from_line}{user_agent}{content_type}{content_length}{cookie_header}{CRLF}{content}"

        so = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock = ssl.wrap_socket(so)
        sock.connect((self.server, self.port))
        sock.send(request.encode())
        response_data = sock.recv(BUFFER).decode()
        sock.close()

        return response_data

    def run(self):
        self.login()

        self.queue.append(f"http://{self.server}/fakebook/")

        flags_founded = False

        while len(self.queue) > 0 and not flags_founded: #if there are still pages to crawl...
            print("COLA: " , self.queue)
            page = self.queue.popleft()

            #send the http request and read the response
            print("HASTA AQUI")
            response = self.send_get_and_receive(page)

            if response != -1:
                #parse the response into a dictonary
                response_dict = self.parse_response(response)

                if response_dict['status'] == SUCCESS:
                    #parse, feed queue and add flags if founded
                    custom_parser = CustomHTMLParser(self)
                    custom_parser.feed(response_dict['body'])
                        
                    if len(self.flags) == 5:
                        flags_founded = True

                elif response_dict['status'] == FOUND:
                    print("FOUND")
                    self.feed_queue(response_dict['headers']['Location'])
                    self.visited.add(response_dict['headers']['Location'])

                elif response_dict['status'] == FORBIDDEN or response_dict['status'] == NOT_FOUND:
                    #abandon the URL
                    pass

                elif response_dict['status'] == ERROR:
                    #re-try the request until it is successful
                    self.queue.append(page)

                else:
                    print(f"Unrecognized status : {response_dict['status']}")	

            else:
                print('[ERROR]: Cannot Crawl Outside The Target Domain')
            


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
